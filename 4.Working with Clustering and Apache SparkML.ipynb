{"cells": [{"metadata": {}, "cell_type": "code", "source": "try:\n    from pyspark import SparkContext, SparkConf\n    from pyspark.sql import SparkSession\nexcept ImportError as e:\n    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')", "execution_count": 1, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# delete files from previous runs\n!rm -f hmp.parquet*\n\n# download the file containing the data in PARQUET format\n!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n    \n# create a dataframe out of it\ndf = spark.read.parquet('hmp.parquet')\n\n# register a corresponding query table\ndf.createOrReplaceTempView('df')", "execution_count": 3, "outputs": [{"output_type": "stream", "text": "--2020-07-23 11:47:39--  https://github.com/IBM/coursera/raw/master/hmp.parquet\nResolving github.com (github.com)... 140.82.114.3\nConnecting to github.com (github.com)|140.82.114.3|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet [following]\n--2020-07-23 11:47:40--  https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet\nReusing existing connection to github.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet [following]\n--2020-07-23 11:47:40--  https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.8.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.8.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 932997 (911K) [application/octet-stream]\nSaving to: \u2018hmp.parquet\u2019\n\n100%[======================================>] 932,997     --.-K/s   in 0.03s   \n\n2020-07-23 11:47:41 (32.2 MB/s) - \u2018hmp.parquet\u2019 saved [932997/932997]\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, Normalizer\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml import Pipeline\n\nindexer = StringIndexer(inputCol=\"class\", outputCol=\"classIndex\")\nencoder = OneHotEncoder(inputCol=\"classIndex\", outputCol=\"categoryVec\")\nvectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n                                  outputCol=\"features\")\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\n\npipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer])\nmodel = pipeline.fit(df)\nprediction = model.transform(df)\nprediction.show()", "execution_count": 4, "outputs": [{"output_type": "stream", "text": "+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\n|  x|  y|  z|              source|      class|classIndex|   categoryVec|        features|       features_norm|\n+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,52.0,34.0]|[0.19626168224299...|\n| 22| 51| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,51.0,34.0]|[0.20560747663551...|\n| 20| 50| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[20.0,50.0,35.0]|[0.19047619047619...|\n| 22| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,34.0]|[0.20370370370370...|\n| 22| 50| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,50.0,34.0]|[0.20754716981132...|\n| 22| 51| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,51.0,35.0]|[0.20370370370370...|\n| 21| 51| 33|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,51.0,33.0]|[0.2,0.4857142857...|\n| 20| 50| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[20.0,50.0,34.0]|[0.19230769230769...|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,49.0,33.0]|[0.20388349514563...|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,49.0,33.0]|[0.20388349514563...|\n| 20| 51| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[20.0,51.0,35.0]|[0.18867924528301...|\n| 18| 49| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[18.0,49.0,34.0]|[0.17821782178217...|\n| 19| 48| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[19.0,48.0,34.0]|[0.18811881188118...|\n| 16| 53| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[16.0,53.0,34.0]|[0.15533980582524...|\n| 18| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[18.0,52.0,35.0]|[0.17142857142857...|\n| 18| 51| 32|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[18.0,51.0,32.0]|[0.17821782178217...|\n+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\nonly showing top 20 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\n\nkmeans = KMeans(featuresCol = \"features\").setK(14).setSeed(1)\npipeline = Pipeline(stages=[vectorAssembler, kmeans])\nmodel = pipeline.fit(df)\npredictions = model.transform(df)\n\nevaluator = ClusteringEvaluator()\n\nsilhouette = evaluator.evaluate(predictions)\nprint(\"Silhouette with sqaured euclidean distance = \" + str(silhouette))\n", "execution_count": 6, "outputs": [{"output_type": "stream", "text": "Silhouette with sqaured euclidean distance = 0.41244594513295846\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "\nkmeans = KMeans(featuresCol=\"features\").setK(2).setSeed(538009335)\npipeline = Pipeline(stages=[vectorAssembler, kmeans])\nmodel = pipeline.fit(df)\npredictions = model.transform(df)\n\nevaluator = ClusteringEvaluator()\n\nsilhouette = evaluator.evaluate(predictions)\nprint(\"Silhouette with squared euclidean distance = \" + str(silhouette))", "execution_count": 8, "outputs": [{"output_type": "stream", "text": "Silhouette with squared euclidean distance = 0.6875664014387497\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "kmeans = KMeans(featuresCol=\"features_norm\").setK(14).setSeed(1)\npipeline = Pipeline(stages=[vectorAssembler, normalizer, kmeans])\nmodel = pipeline.fit(df)\n\npredictions = model.transform(df)\n\nevaluator = ClusteringEvaluator()\n\nsilhouette = evaluator.evaluate(predictions)\nprint(\"Silhouette with squared euclidean distance = \" + str(silhouette))", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "Silhouette with squared euclidean distance = 0.2668998965895519\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.sql.functions import col\ndf_denormalized = df.select([col('*'),(col('x')*10)]).drop('x').withColumnRenamed('(x * 10)','x')", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "kmeans = KMeans(featuresCol=\"features\").setK(14).setSeed(1)\npipeline = Pipeline(stages=[vectorAssembler, kmeans])\nmodel = pipeline.fit(df_denormalized)\npredictions = model.transform(df_denormalized)\n\nevaluator = ClusteringEvaluator()\n\nsilhouette = evaluator.evaluate(predictions)\nprint(\"Silhouette with squared euclidean distance = \" + str(silhouette))", "execution_count": 11, "outputs": [{"output_type": "stream", "text": "Silhouette with squared euclidean distance = 0.5709023393004293\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml.clustering import GaussianMixture\n\ngmm =  GaussianMixture().setK(2).setSeed(538009335)\npipeline = Pipeline(stages=[vectorAssembler, gmm])\n\nmodel = pipeline.fit(df)\n\npredictions = model.transform(df)\n\nevaluator = ClusteringEvaluator()\n\nsilhouette = evaluator.evaluate(predictions)\nprint(\"Silhouette with squared euclidean distance = \" + str(silhouette))", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}