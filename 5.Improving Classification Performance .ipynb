{"cells": [{"metadata": {}, "cell_type": "code", "source": "try:\n    from pyspark import SparkContext, SparkConf\n    from pyspark.sql import SparkSession\nexcept ImportError as e:\n    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')", "execution_count": 1, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\n# delete files from previous runs\n!rm -f hmp.parquet*\n\n# download the file containing the data in PARQUET format\n!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n    \n# create a dataframe out of it\ndf = spark.read.parquet('hmp.parquet')\n\n# register a corresponding query table\ndf.createOrReplaceTempView('df')\n", "execution_count": 3, "outputs": [{"output_type": "stream", "text": "--2020-07-23 12:16:40--  https://github.com/IBM/coursera/raw/master/hmp.parquet\nResolving github.com (github.com)... 140.82.112.3\nConnecting to github.com (github.com)|140.82.112.3|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet [following]\n--2020-07-23 12:16:41--  https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet\nReusing existing connection to github.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet [following]\n--2020-07-23 12:16:41--  https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.8.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.8.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 932997 (911K) [application/octet-stream]\nSaving to: \u2018hmp.parquet\u2019\n\n100%[======================================>] 932,997     --.-K/s   in 0.04s   \n\n2020-07-23 12:16:41 (23.8 MB/s) - \u2018hmp.parquet\u2019 saved [932997/932997]\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "#splitting in test and train\nsplits = df.randomSplit([0.8, 0.2])\ndf_train = splits[0]\ndf_test = splits[1]", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml.feature import StringIndexer, OneHotEncoder\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler \nfrom pyspark.ml.feature import Normalizer\n\nindexer = StringIndexer(inputCol = 'class', outputCol = 'label')\n\nvectorAssembler = VectorAssembler(inputCols = ['x','y','z'],\n                                  outputCol = 'features')\nnormalizer = Normalizer(inputCol = 'features', outputCol = 'features_norm', p=1.0)\n", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\n\nlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\npipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer,lr])\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)", "execution_count": 12, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "prediction.printSchema()", "execution_count": 13, "outputs": [{"output_type": "stream", "text": "root\n |-- x: integer (nullable = true)\n |-- y: integer (nullable = true)\n |-- z: integer (nullable = true)\n |-- source: string (nullable = true)\n |-- class: string (nullable = true)\n |-- label: double (nullable = false)\n |-- features: vector (nullable = true)\n |-- features_norm: vector (nullable = true)\n |-- rawPrediction: vector (nullable = true)\n |-- probability: vector (nullable = true)\n |-- prediction: double (nullable = false)\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\nMulticlassClassificationEvaluator().setMetricName(\"accuracy\").evaluate(prediction)", "execution_count": 14, "outputs": [{"output_type": "execute_result", "execution_count": 14, "data": {"text/plain": "0.20764125160354732"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml.classification import RandomForestClassifier\nrf = RandomForestClassifier(labelCol ='label', featuresCol = 'features', numTrees = 10)\npipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer, rf])\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)", "execution_count": 15, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "MulticlassClassificationEvaluator().setMetricName('accuracy').evaluate(prediction)", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}